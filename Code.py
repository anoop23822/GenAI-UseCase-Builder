# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejpeYINN0-8kJQppiONy3oigjAw0RBVc
"""

# Import necessary libraries for web scraping, HuggingFace transformers, and dataset collection
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
from google.colab import files

# Function to research the industry or company by scraping Google News
def research_industry(company_name):
    query = f"{company_name} industry trends"
    search_url = f"https://news.google.com/search?q={query}&hl=en-IN&gl=IN&ceid=IN%3Aen"

    response = requests.get(search_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extracting article titles and links from Google News
    articles = []
    for result in soup.find_all('article'):
        headline = result.find('h3')
        if headline:
            title = headline.get_text()
            link = result.find_parent('a')['href']
            articles.append({'title': title, 'link': 'https://news.google.com' + link})

    return articles

# Function to generate use cases using HuggingFace's pre-trained models (free models)
def generate_use_cases(industry_data):
    generator = pipeline('text-generation', model='gpt2')  # GPT-2 (free to use)

    # Creating a prompt from the industry data to generate relevant use cases
    prompt = f"Given the industry trends and technologies in the following data, propose some AI/ML use cases: {industry_data}"

    # Generate use cases using GPT-2
    use_case_text = generator(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']
    return use_case_text

# Function to collect publicly available datasets (from Kaggle and HuggingFace)
def collect_datasets():
    datasets = {
        'Kaggle': 'https://www.kaggle.com/datasets',
        'HuggingFace': 'https://huggingface.co/datasets',
    }
    return datasets

# Main function that ties everything together
def main(companies_list):
    # Open file for writing
    with open("companies_use_cases_and_datasets.md", 'w') as file:
        # Loop through each company in the list
        for company_name in companies_list:
            file.write(f"\n## Industry Research for {company_name}:\n")
            # Step 1: Research the Industry
            print(f"Researching industry data for {company_name}...")
            industry_data = research_industry(company_name)
            for article in industry_data:
                file.write(f"- {article['title']} [{article['link']}]\n")

            # Step 2: Generate Use Cases using a free HuggingFace model
            print(f"Generating AI/ML use cases for {company_name}...")
            industry_info = " ".join([article['title'] for article in industry_data])  # Concatenating article titles as context
            use_cases = generate_use_cases(industry_info)
            file.write(f"\n## Generated AI/ML Use Cases for {company_name}:\n{use_cases}\n")

            # Step 3: Collect relevant datasets from Kaggle & HuggingFace
            print(f"Collecting datasets for {company_name}...")
            datasets = collect_datasets()
            file.write(f"\n## Datasets for {company_name}:\n")
            for source, link in datasets.items():
                file.write(f"- [{source}]({link})\n")

        print("All companies processed. File saved as 'companies_use_cases_and_datasets.md'.")

# List of 30 companies (replace with actual company names)
companies = [
    'Tesla', 'Amazon', 'Google', 'Microsoft', 'Apple', 'Facebook', 'Netflix', 'Salesforce', 'IBM', 'Adobe',
    'Nike', 'Toyota', 'Volkswagen', 'Ford', 'General Motors', 'Intel', 'Nvidia', 'Samsung', 'Uber', 'Lyft',
    'Spotify', 'Zoom', 'Snapchat', 'Twitter', 'LinkedIn', 'PayPal', 'Square', 'Pinterest', 'Dropbox', 'Slack'
]

# Run the workflow for all companies in the list
main(companies)

# Allow the user to download the result file
files.download("companies_use_cases_and_datasets.md")

